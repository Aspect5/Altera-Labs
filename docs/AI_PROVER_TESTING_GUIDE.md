# AI Prover Testing Guide - Altera Labs

## üéØ Quick Start Testing

### **1. Run the Demo**
```bash
cd /workspaces/Altera-Labs/backend
python demo_ai_prover.py
```

### **2. Test Individual Components**
```bash
# Test LLM Performance Tester
python -c "from llm_performance_tester import LLMPerformanceTester; tester = LLMPerformanceTester(); print(f'‚úÖ Loaded {len(tester.test_cases)} test cases')"

# Test Natural Language Translation
python -c "from socratic_auditor import get_llm_response; response = get_llm_response('Translate to Lean tactic: use reflexivity for 1 + 1 = 2'); print(response)"

# Test Lean Verification
cd lean_verifier && lake build
```

## üìä Current Capabilities Summary

### **‚úÖ What's Working (100% Success Rate)**
- **Basic Arithmetic**: Addition, multiplication, identity properties
- **Commutativity**: Addition and multiplication commutativity
- **Associativity**: Addition associativity
- **Natural Language Translation**: Simple tactic generation
- **Lean 4 Integration**: Formal verification with Mathlib
- **Performance Testing**: Comprehensive test suite with metrics

### **üìà Performance Metrics**
- **Success Rate**: 100% (6/6) on medium test suite
- **Average Time**: 15.92s per test
- **Average Attempts**: 1.0 (perfect efficiency)
- **LLM Response Time**: ~2-3s per call
- **Lean Verification Time**: ~12-13s per verification

## üß™ Testing Infrastructure

### **1. Test Suites Available**

#### **LLM Performance Tester** (`llm_performance_tester.py`)
- **12 test cases** covering arithmetic, logic, and advanced domains
- **Difficulty levels**: Easy (3), Medium (7), Hard (2)
- **Categories**: Arithmetic (6), Inequality (2), Logic (2), Advanced (2)

#### **Hard Test Suite** (`hard_test_suite.py`)
- **Advanced mathematical challenges**
- Exponential properties, binomial theorem, De Morgan's laws
- Group theory theorems, Fibonacci properties

#### **Simple Hard Tests** (`simple_hard_tests.py`)
- **Optimized for available Lean imports**
- Distributive law, successor properties, logic fundamentals

### **2. API Testing Endpoints**

#### **Performance Testing**
```bash
# Run complete test suite
curl -X POST http://localhost:5000/api/performance/run_tests

# Get all test cases
curl -X GET http://localhost:5000/api/performance/test_cases

# Run single test case
curl -X POST http://localhost:5000/api/performance/run_single_test \
  -H "Content-Type: application/json" \
  -d '{"test_name": "Simple Addition"}'

# Get performance reports
curl -X GET http://localhost:5000/api/performance/reports
```

#### **Mathematical Verification**
```bash
# Verify mathematical step
curl -X POST http://localhost:5000/api/verify_step \
  -H "Content-Type: application/json" \
  -d '{
    "proof_state": "theorem test : 1 + 1 = 2 := by sorry",
    "user_message": "use reflexivity"
  }'

# Auto-solve proof
curl -X POST http://localhost:5000/api/auto_solve_proof \
  -H "Content-Type: application/json" \
  -d '{
    "proof_state": "theorem test : ‚àÄ n, n + 0 = n := by sorry"
  }'

# Chat with mathematical verification
curl -X POST http://localhost:5000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "prove that addition is commutative"}'
```

## üéØ Testing Scenarios

### **1. Basic Functionality Testing**

#### **Natural Language Translation**
```python
# Test simple reflexivity
response = get_llm_response("Translate to Lean tactic: use reflexivity for 1 + 1 = 2")
# Expected: Should generate "rfl"

# Test multi-step tactics
response = get_llm_response("Translate to Lean tactic: introduce variables and use commutativity for ‚àÄ a b, a + b = b + a")
# Expected: Should generate "intro a b; exact Nat.add_comm a b"
```

#### **Proof Verification**
```python
# Test simple proof
proof_state = """import Mathlib.Data.Nat.Basic
theorem test : 1 + 1 = 2 := by sorry"""
new_state = proof_state.replace("sorry", "rfl")
# Expected: Should verify successfully with Lean 4
```

### **2. Performance Testing**

#### **Run Complete Test Suite**
```bash
python -c "
from llm_performance_tester import LLMPerformanceTester
tester = LLMPerformanceTester()
report = tester.run_full_test_suite()
print(f'Success Rate: {report.success_rate:.1%}')
print(f'Average Time: {report.average_time:.2f}s')
print(f'Total Tests: {report.total_tests}')
"
```

#### **Test Individual Cases**
```bash
python -c "
from llm_performance_tester import LLMPerformanceTester
tester = LLMPerformanceTester()
test_case = tester.test_cases[0]  # Simple Addition
result = tester.run_single_test(test_case)
print(f'Test: {test_case.name}')
print(f'Success: {result.success}')
print(f'Time: {result.total_time:.2f}s')
print(f'Attempts: {result.attempts_used}')
"
```

### **3. Error Handling Testing**

#### **Invalid Inputs**
```python
# Test with invalid Lean syntax
invalid_proof = """import Mathlib.Data.Nat.Basic
theorem test : 1 + 1 = 2 := by invalid_tactic"""

# Test with timeout scenarios
# Test with malformed natural language
```

#### **Edge Cases**
```python
# Test empty inputs
# Test very long proofs
# Test complex mathematical domains
```

### **4. Integration Testing**

#### **End-to-End Workflow**
```python
# 1. Start session
# 2. Upload mathematical problem
# 3. Generate proof steps
# 4. Verify each step
# 5. Complete proof
# 6. Generate feedback
```

#### **Concurrent Testing**
```bash
# Test multiple simultaneous requests
for i in {1..5}; do
  curl -X POST http://localhost:5000/api/performance/run_single_test \
    -H "Content-Type: application/json" \
    -d '{"test_name": "Simple Addition"}' &
done
wait
```

## üìã Testing Checklist

### **‚úÖ Core Functionality**
- [ ] Natural language to Lean translation works
- [ ] Proof verification with Lean 4 succeeds
- [ ] Error handling for invalid inputs
- [ ] Multi-step proof handling
- [ ] Auto-solving capability

### **‚úÖ Performance**
- [ ] Response times under 20 seconds
- [ ] Success rate above 90%
- [ ] No memory leaks or crashes
- [ ] Concurrent request handling
- [ ] Timeout handling

### **‚úÖ API Endpoints**
- [ ] All performance testing endpoints respond
- [ ] Mathematical verification endpoints work
- [ ] Error responses are meaningful
- [ ] JSON responses are valid
- [ ] Authentication/authorization (if applicable)

### **‚úÖ Mathematical Domains**
- [ ] Basic arithmetic (addition, multiplication)
- [ ] Identity properties (zero, one)
- [ ] Commutativity and associativity
- [ ] Propositional logic
- [ ] Advanced mathematics (group theory, etc.)

## üîç Debugging and Troubleshooting

### **Common Issues**

#### **1. Lean 4 Build Failures**
```bash
# Check Lean installation
lean --version

# Rebuild Lean project
cd lean_verifier && lake build

# Check for missing imports
lake lean LeanVerifier/Basic.lean
```

#### **2. LLM API Issues**
```bash
# Check environment variables
echo $VERTEX_AI_PROJECT_ID
echo $VERTEX_AI_LOCATION
echo $DEFAULT_LLM_MODEL

# Test LLM connection
python -c "from socratic_auditor import get_llm_response; print(get_llm_response('test'))"
```

#### **3. Performance Issues**
```bash
# Check system resources
top
free -h

# Monitor Lean verification times
# Check for memory leaks
```

### **Logging and Monitoring**

#### **Enable Debug Logging**
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

#### **Monitor Performance**
```python
import time
start_time = time.time()
# ... test code ...
print(f"Time taken: {time.time() - start_time:.2f}s")
```

## üöÄ Advanced Testing

### **1. Stress Testing**
```bash
# Run multiple test suites simultaneously
python hard_test_suite.py &
python simple_hard_tests.py &
wait
```

### **2. Regression Testing**
```bash
# Compare results with baseline
python -c "
from llm_performance_tester import LLMPerformanceTester
tester = LLMPerformanceTester()
baseline_results = tester.run_full_test_suite()
# Save baseline for comparison
"

# Run tests and compare
python -c "
# Load baseline and compare with current results
"
```

### **3. Mathematical Domain Testing**
```bash
# Test specific mathematical domains
python -c "
# Test arithmetic domain
# Test logic domain
# Test advanced mathematics
"
```

## üìä Reporting and Analysis

### **Generate Performance Reports**
```bash
# Run tests and generate report
python -c "
from llm_performance_tester import LLMPerformanceTester
tester = LLMPerformanceTester()
report = tester.run_full_test_suite()
tester.save_report(report, 'performance_report.md')
print('Report saved as performance_report.md')
"
```

### **Analyze Test Results**
```bash
# Analyze success rates by difficulty
# Analyze timing patterns
# Identify bottlenecks
# Generate recommendations
```

## üéØ Next Steps

### **Immediate Testing Priorities**
1. **Run the demo script** to verify basic functionality
2. **Test individual components** to ensure they work correctly
3. **Run performance tests** to establish baseline metrics
4. **Test error handling** with invalid inputs
5. **Verify API endpoints** are working correctly

### **Advanced Testing Opportunities**
1. **Expand test coverage** to more mathematical domains
2. **Implement automated regression testing**
3. **Add performance benchmarking**
4. **Create user acceptance tests**
5. **Develop stress testing scenarios**

### **Continuous Improvement**
1. **Monitor performance metrics** over time
2. **Analyze error patterns** and improve error handling
3. **Optimize LLM prompts** based on test results
4. **Expand mathematical domain coverage**
5. **Improve pedagogical feedback quality**

---

## üìû Support and Resources

- **Documentation**: `AI_PROVER_CAPABILITIES_ANALYSIS.md`
- **Demo Script**: `demo_ai_prover.py`
- **Performance Report**: `LLM_PERFORMANCE_REPORT.md`
- **Test Suites**: `llm_performance_tester.py`, `hard_test_suite.py`, `simple_hard_tests.py`

The AI prover system is ready for comprehensive testing and demonstrates world-class capabilities in mathematical proof verification! 